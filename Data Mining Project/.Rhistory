registerDoParallel(clusters)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/lassoVarImp.csv")
sigVars <- sigVars[, 2]
sigVars <- as.vector(sigVars)
label <- dtaMining[, 1]
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
train_2 <- ifelse(train_2 == 1, "att", "nos")
xgb_grid_1 <- expand.grid(
nrounds = 1000,
eta = c(0.01, 0.001, 0.0001),
max_depth = c(2, 4, 6, 8, 10),
gamma = 1,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 0.5
)
# pack the training control parameters
xgb_trcontrol_1 <- trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                                                        # save losses across all models
classProbs = TRUE,                                                           # set to TRUE for AUC to be computed
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 <- train(
x = as.matrix(train_1),
y = as.factor(train_2),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
allowParallel = TRUE
)
xgb_train_1
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(glmnet)
library(doParallel)
set.seed(1234)
clusters <- makeCluster(detectCores())
registerDoParallel(clusters)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
names(dtaMining)
rm(list = ls())
library(tidyverse)
library(dplyr)
library(lubridate)
library(data.table)
library(caret)
memory.limit(50000)
regionData <- read.csv2("./dta/Regional Grouping.csv", sep = ",", na.strings = c(" ", ""))  ### data for regional groups
names(regionData)[1] <- "Country"
regionData[] <- lapply(regionData, function(x) tolower(x))
#codeData <- read.csv2("./Source Codes.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes
dta_1 <- read.csv2("./dta/BIG5 ATD 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for attended
dta_1$label <- 1
dta_2 <- read.csv2("./dta/BIG5 NS 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for no shows
dta_2$label <- 0
dta_3 <- rbind(dta_1, dta_2)
dta_3 <- setDT(dta_3) ## SET AS DATA.TABLE
dta_3[] <- lapply(dta_3, function(x) as.character(x))
dta_3[] <- lapply(dta_3, function(x) tolower(x))
dta_3 <- dta_3[, c(138, 1:137)]
names <- grep("Attend", names(dta_3), value = TRUE)
dta_3 <- dta_3[, -as.vector(names), with = FALSE]
names(dta_3)
dta[1:100, 37]
dta_3[1:100, 37]
dta_3[1:1000, 37]
table(dta_3[, 37])
names(dta_3)
table(dta_3[, 79])
names(dta_1)
table(dta_3[, 122])
table(dta_3[, 123])
table(dta_3[, 124])
names(dta_3[, 124])
names(dta_1[, 124])
names(dta_1)
table(dta_1[,123])
table(dta_1[,124])
summary(dta_1[,123:124])
names(dta_1)
summary(dta_1[,125])
summary(dta_1[,35])
summary(dta_2[,35])
dta_1 <- read.csv2("./dta/BIG5 ATD 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for attended
dta_1$label <- 1
dta_1 <- dta_1[dta_1$Show.Registered.For == "BIG]",]
dta_2 <- read.csv2("./dta/BIG5 NS 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for no shows
dta_2$label <- 0
dta_2 <- dta_2[dta_2$Show.Registered.For == "BIG]",]
dta_1 <- read.csv2("./dta/BIG5 ATD 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for attended
dta_1$label <- 1
#dta_1 <- dta_1[dta_1$Show.Registered.For == "BIG]",]
dta_2 <- read.csv2("./dta/BIG5 NS 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for no shows
dta_2$label <- 0
## DATE : 10/16/2017
## AUTHOR  : GREJELL B. SEGURA
## THIS SCRIPT WILL PREPARE THE DATA FOR DATA ANALYSIS OF BIG 5 2016
## Part 1 ##
rm(list = ls())
library(tidyverse)
library(dplyr)
library(lubridate)
library(data.table)
library(caret)
memory.limit(50000)
regionData <- read.csv2("./dta/Regional Grouping.csv", sep = ",", na.strings = c(" ", ""))  ### data for regional groups
names(regionData)[1] <- "Country"
regionData[] <- lapply(regionData, function(x) tolower(x))
#codeData <- read.csv2("./Source Codes.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes
dta_1 <- read.csv2("./dta/BIG5 ATD 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for attended
dta_1$label <- 1
dta_1 <- dta_1[dta_1$Show.Registered.For == "BIG]",]
dta_2 <- read.csv2("./dta/BIG5 NS 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for no shows
dta_2$label <- 0
dta_2 <- dta_2[dta_2$Show.Registered.For == "BIG]",]
dta_3 <- rbind(dta_1, dta_2)
dta_3 <- setDT(dta_3) ## SET AS DATA.TABLE
dta_3[] <- lapply(dta_3, function(x) as.character(x))
dta_3[] <- lapply(dta_3, function(x) tolower(x))
dta_3 <- dta_3[, c(138, 1:137)]
names <- grep("Attend", names(dta_3), value = TRUE)
dta_3 <- dta_3[, -as.vector(names), with = FALSE]
## TRANSFORM DATA FROM WIDE TO LONG
dta_3 <- melt(dta_3, measure.vars = names(dta_3)[36:length(dta_3)], variable.name = "attribute", value.name = "value")
## remove unnecessary features
dta_3 <- dta_3[!(is.na(value)),]
dta_3[] <- lapply(dta_3, function(x) as.character(x))
na <- colSums(is.na(dta_3))/nrow(dta_3)
na <- ifelse(na > .9, FALSE, TRUE)
dta_3 <- dta_3[, na, with = FALSE]
## remove special characters in names, position, company, id
names(dta_3)[2] <- "id"
dta_3[, c(3,4,6,7)] <- lapply(dta_3[, c(3,4,6,7), with = FALSE], function(x) gsub("[^A-z]", "", x))
dta_3[, 2] <- lapply(dta_3[, 2,with = FALSE], function(x) gsub("[^0-9]", "", x))
dta_3[, 6] <- lapply(dta_3[, 6,with = FALSE], function(x) gsub("[^A-z0-9]", "", x))
dta_3[] <- lapply(dta_3, function(x) gsub("^$", NA, x)) ## REPLACE BLANKS WITH NA
dta_3 <- dta_3[, -c(3,8,9,10,11,13,14,15,16,17,18,19,20,21,24)]
## separate "value" into columns by "]"
names <- paste0("v", 1:33)
dta_4 <- dta_3[, c(2, 11)]
dta_4[, c(names) := tstrsplit(value, "]", fixed = TRUE)]
dta_4 <- dta_4[, -2]
dta_4 <- melt(dta_4, measure.vars = names, variable.name = "attribute", value.name = "value")
dta_4 <- dta_4[, -c("attribute")]
dta_4 <- dta_4[!(is.na(value)), ]
dta_3 <- dta_3[, -c("attribute","value")]
dta_3 <- unique(dta_3)
dta_3 <- merge(dta_3, dta_4, by = "id", all = TRUE)
dta_3 <- merge(dta_3, regionData, by = "Country", all.x = TRUE)
## format Dates
dta_3[, c("Date", "Time", "am.pm") := tstrsplit(Date.Created, " ", fixed = TRUE)]
dta_3$Date <- dmy(dta_3$Date)
dta_clean <- dta_3[, c(1,2,3,4,5,6,9,10,11,12,13)]
write.csv(dta_clean, "./dta/big5_cleanData.csv", row.names = FALSE)
##############################################################################################################
## Part 2 ##
rm(list = ls()) ## clear the memory
codeData <- read.csv2("./dta/BIG5 CODES 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes
codeData[] <- lapply(codeData, function(x) tolower(x))
codeData[] <- lapply(codeData, function(x) gsub(" ", ".", x))
codeData[] <- lapply(codeData, function(x) gsub("[^A-z0-9]", "", x))
varnames1 <- grepl("dtcm", codeData[, 1])
codeData <- codeData[varnames1 == "FALSE", ]
codeData <- codeData[, -1]
codeData <- codeData[codeData$usage == "demographic", ]
codeData <- codeData[, -3]
names(codeData) <- c("value", "value.1")
dtaMiningRaw <- read.csv2("./dta/big5_cleanData.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes
dtaMining <- setDT(dtaMiningRaw)
# remove unwanted features
dtaMining <- dtaMining[, -c(4,5,6,7,9)]
dtaMining[] <- lapply(dtaMining, function(x) as.character(x))
# convert to characters
dtaMining$Country[is.na(dtaMining$Country)] <- "none"
dtaMining$Region.2[is.na(dtaMining$Region.2)] <- "none"
# format Date and create no.days to show
dtaMining$Date <- ymd(dtaMining$Date)
dtaMining$daysToShow <- ymd("2016-11-21") - dtaMining$Date
dtaMining$daysToShow <- as.numeric(dtaMining$daysToShow)
# remove those who registered on the event period
dtaMining <- dtaMining[daysToShow > 0, ]
# remove Date
dtaMining <- dtaMining[, -6]
dtaMining <- dtaMining[, -1]
# replace codes with identifiable terms
codeData <- unique(codeData)
dtaMining <- merge(dtaMining, codeData, by = "value", all.x = TRUE)
dtaMining2 <- dtaMining
dtaMining <- dtaMining2[, -c("value")]
dtaMining$value <- dtaMining$value.1
dtaMining <- dtaMining[, -c("value.1")]
# convert to wide
dtaMining <- dcast(dtaMining, id + label + Region.2 + daysToShow ~ value)
dtaMining <- dtaMining[, -1]
dummy <- dummyVars(~., dtaMining)
dummy_dta <- predict(dummy, dtaMining)
dtaMining2 <- as.data.frame(dummy_dta)
dtaMining <- dtaMining2
varnames1 <- grep("attd", names(dtaMining), value = TRUE)
#dtaMining <- dtaMining[, -as.vector(varnames1), with = FALSE]
dtaMining <- dtaMining[, -c("NA")]
nms <- names(dtaMining[, -1])
nms <- lapply(nms, function(x) paste0("a",x,"a", collapse = ""))
names(dtaMining)[2:length(dtaMining)] <- as.character(nms)
dtaMining <- dtaMining[, -(length(dtaMining))]
names(dtaMining)[c(11:21)] <- c(paste("a", 11:21, sep = ""))
write.csv(dtaMining, "./dta/dtaminingClean.csv", row.names = FALSE)
rm(list = ls())
library(tidyverse)
library(dplyr)
library(lubridate)
library(data.table)
library(caret)
library(lmtest)
memory.limit(50000)
## PART 1 - PREPROCESS
##  THE PREPROCESS FOR LOGISTIC REGRESSION IS DIFFERENT FROM OTHER MACHINE LEARNING TO AVOID "DUMMY TRAP" ON THE FEATURES ##
regionData <- read.csv2("./dta/Regional Grouping.csv", sep = ",", na.strings = c(" ", ""))  ### data for regional groups
names(regionData)[1] <- "Country"
regionData[] <- lapply(regionData, function(x) tolower(x))
#codeData <- read.csv2("./Source Codes.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes
dta_1 <- read.csv2("./dta/BIG5 ATD 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for attended
dta_1$label <- 1
dta_1 <- dta_1[dta_1$Show.Registered.For == "BIG]",]
dta_2 <- read.csv2("./dta/BIG5 NS 2016.csv", sep = ",", na.strings = c(" ", ""))  ### data for no shows
dta_2$label <- 0
dta_2 <- dta_2[dta_2$Show.Registered.For == "BIG]",]
dta_3 <- rbind(dta_1, dta_2)
dta_3 <- setDT(dta_3) ## SET AS DATA.TABLE
dta_3[] <- lapply(dta_3, function(x) as.character(x))
dta_3[] <- lapply(dta_3, function(x) tolower(x))
dta_3 <- dta_3[, c(138, 1:137)]
names <- grep("Attend", names(dta_3), value = TRUE)
dta_3 <- dta_3[, -as.vector(names), with = FALSE]
## TRANSFORM DATA FROM WIDE TO LONG
dta_3 <- melt(dta_3, measure.vars = names(dta_3)[36:length(dta_3)], variable.name = "attribute", value.name = "value")
## remove unnecessary features
dta_3 <- dta_3[!(is.na(value)),]
dta_3[] <- lapply(dta_3, function(x) as.character(x))
na <- colSums(is.na(dta_3))/nrow(dta_3)
na <- ifelse(na > .9, FALSE, TRUE)
dta_3 <- dta_3[, na, with = FALSE]
## remove special characters in names, position, company, id
names(dta_3)[2] <- "id"
dta_3[, c(3,4,6,7)] <- lapply(dta_3[, c(3,4,6,7), with = FALSE], function(x) gsub("[^A-z]", "", x))
dta_3[, 2] <- lapply(dta_3[, 2,with = FALSE], function(x) gsub("[^0-9]", "", x))
dta_3[, 6] <- lapply(dta_3[, 6,with = FALSE], function(x) gsub("[^A-z0-9]", "", x))
dta_3[] <- lapply(dta_3, function(x) gsub("^$", NA, x)) ## REPLACE BLANKS WITH NA
dta_3 <- dta_3[, -c(3,8,9,10,11,13,14,15,16,17,18,19,20,21,24)]
## separate "value" into columns by "]"
names <- paste0("v", 1:33)
dta_4 <- dta_3[, c(2, 11)]
dta_4[, c(names) := tstrsplit(value, "]", fixed = TRUE)]
dta_4 <- dta_4[, -2]
dta_4 <- melt(dta_4, measure.vars = names, variable.name = "attribute", value.name = "value")
dta_4 <- dta_4[, -c("attribute")]
dta_4 <- dta_4[!(is.na(value)), ]
dta_3 <- dta_3[, -c("attribute","value")]
dta_3 <- unique(dta_3)
dta_3 <- merge(dta_3, dta_4, by = "id", all = TRUE)
dta_3 <- merge(dta_3, regionData, by = "Country", all.x = TRUE)
## format Dates
dta_3[, c("Date", "Time", "am.pm") := tstrsplit(Date.Created, " ", fixed = TRUE)]
dta_3$Date <- dmy(dta_3$Date)
dta_clean <- dta_3[, c(1,2,3,4,5,6,9,10,11,12,13)]
write.csv(dta_clean, "./dta/big5_cleanData.csv", row.names = FALSE)
##############################################################################################################
## Part 2 ##
rm(list = ls()) ## clear the memory
codeData <- read.csv2("./dta/BIG5 CODES 2016 - logit v3.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes - THIS IS DIFFERENT FROM THE MACHINE LEARNING CODES
codeData[] <- lapply(codeData, function(x) tolower(x))
codeData[] <- lapply(codeData, function(x) gsub(" ", ".", x))
codeData[] <- lapply(codeData, function(x) gsub("[^A-z0-9]", "", x))
varnames1 <- grepl("dtcm", codeData[, 1])
codeData <- codeData[varnames1 == "FALSE", ]
codeData <- codeData[, -1]
codeData <- codeData[codeData$usage == "demographic", ]
codeData <- codeData[codeData$Decode != "dummyvar", ]
codeData <- codeData[, -3]
names(codeData) <- c("value", "value.1")
dtaMiningRaw <- read.csv2("./dta/big5_cleanData.csv", sep = ",", na.strings = c(" ", ""))  ### data for codes
dtaMining <- setDT(dtaMiningRaw)
# remove unwanted features
dtaMining <- dtaMining[, -c(4,5,6,7,9)]
dtaMining[] <- lapply(dtaMining, function(x) as.character(x))
# convert to characters
dtaMining$Country[is.na(dtaMining$Country)] <- "none"
dtaMining$Region.2[is.na(dtaMining$Region.2)] <- "none"
# format Date and create no.days to show
dtaMining$Date <- ymd(dtaMining$Date)
dtaMining$daysToShow <- ymd("2016-11-21") - dtaMining$Date
dtaMining$daysToShow <- as.numeric(dtaMining$daysToShow)
# remove those who registered on the event period
dtaMining <- dtaMining[daysToShow > 0, ]
# remove Date
dtaMining <- dtaMining[, -6]
dtaMining <- dtaMining[, -1]
# replace codes with identifiable terms
codeData <- unique(codeData)
dtaMining <- merge(dtaMining, codeData, by = "value", all.x = TRUE)
dtaMining2 <- dtaMining
dtaMining <- dtaMining2[, -c("value")]
dtaMining$value <- dtaMining$value.1
dtaMining <- dtaMining[, -c("value.1")]
# convert to wide
dtaMining <- dcast(dtaMining, id + label + Region.2 + daysToShow ~ value)
dtaMining <- dtaMining[, -1]
dummy <- dummyVars(~., dtaMining)
dummy_dta <- predict(dummy, dtaMining)
dtaMining2 <- as.data.frame(dummy_dta)
dtaMining <- dtaMining2
varnames1 <- grep("attd", names(dtaMining), value = TRUE)
#dtaMining <- dtaMining[, -as.vector(varnames1), with = FALSE]
dtaMining <- dtaMining[, -(max(length(names(dtaMining))))]
dtaMining <- dtaMining[, -c(1, 9)]
write.csv(dtaMining, "./dta/dtaminingClean_for_logisticRegression.csv", row.names = FALSE)
##############################################################################################################################
##############################################################################################################################
##############################################################################################################################
## PART 2 - LOGISTIC REGRESSION
rm(list = ls())
dta <- read.csv("./dta/dtaminingClean_for_logisticRegression.csv")
names(dta)[1] <- "label"
logitModel_1 <- glm(label ~., dta, family = binomial(link = "logit"))
important_vars <- varImp(logitModel_1)
## significant vars only
significantVars <- coef(summary(logitModel_1))
significantVars <- as.data.frame(significantVars)
significantVars <- significantVars[significantVars[, "Pr(>|z|)"] < 0.1, ]
write.csv(significantVars, "./dta/SignificantVariables.csv")
## only the significant variables will be used in the next models
significantVars
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
'sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)'
names(dtaMining)[1] <- "label"
'dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]'
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
# PART 2 - LOGISTIC REGRESSION
rm(list = ls())
dta <- read.csv("./dta/dtaminingClean_for_logisticRegression.csv")
names(dta)[1] <- "label"
logitModel_1 <- glm(label ~., dta, family = binomial(link = "logit"))
summary(logitModel_1)
