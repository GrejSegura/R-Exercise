rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)

## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE

dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
label <- dtaMining[, 1]
dtaMining <- dtaMining[, sigVars, with = FALSE]
dtaMining <- dtaMining[, -110]

names(dtaMining)[1] <- "label"

d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))



#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]

write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)

## XGBOOST ---

train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]

train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]

length(train_2)

train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)


xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2), 
		   booster = 'gbtree',
		   objective = 'multi:softmax',
		   num_class = 2,
		   max.depth = 8,
		   eta = 10/500,
		   nthread = 8,
		   nrounds = 1000,
		   min_child_weight = 3/(nrow(train_dta[train_dta$label == "1",])/nrow(train_dta)),
		   subsample = 0.5,
		   colsample_bytree = 1, 
		   num_parallel_tree = 1)

#predict gbtree ----------------

pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))

save(xgModel, "./xgModel.rda")
write.csv(pred_xg_train, "./dta/pred_xg_train.csv", row.names = FALSE)
write.csv(pred_xg_test, "./dta/pred_xg_test.csv", row.names = FALSE)

##compute accuracy
rm(list = ls())
library(tidyverse)
library(dplyr)
library(lubridate)
library(data.table)
library(caret)
library(lmtest)
memory.limit(50000)

train <- fread("./dta/train.csv")
test <- fread("./dta/test.csv")

train$label <- as.numeric(train$label)
test$label <- as.numeric(test$label)

logitModel <- glm(label ~., data = train, family = binomial(link = "logit"))

pred_lg_train <- predict(logitModel, train)
pred_lg_train <- ifelse(pred_logit > 0.5, 1, 0)

pred_lg_test <- predict(logitModel_1, test)
pred_lg_test <- ifelse(pred_logit > 0.5, 1, 0)

save(logitModel, "./logitModel.rda")
write.csv(pred_lg_train, "./dta/pred_lg_train.csv", row.names = FALSE)
write.csv(pred_lg_test, "./dta/pred_lg_test.csv", row.names = FALSE)


rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
memory.limit(50000)

train <- fread("./dta/train.csv")
test <- fread("./dta/test.csv")

train$label <- as.factor(train$label)
test$label <- as.factor(test$label)

train$daysToShow <- as.numeric(train$daysToShow)

rf_model <- randomForest(label ~ ., data = as.data.frame(train), ntree = 120, importance = TRUE)

# prediction and accurancy
pred_rf_train <- predict(rf_model, train)
pred_rf_train <- as.numeric(pred_rf_train)
pred_rf_train <- ifelse(pred_rf_train == 2, 1, 0)

pred_rf_test <- predict(rf_model, test)
pred_rf_test <- as.numeric(pred_rf_test)
pred_rf_test <- ifelse(pred_rf_test == 2, 1, 0)

save(rf_model, "./rfmodel.rda")
write.csv(pred_rf_train, "./dta/pred_rf_train.csv", row.names = FALSE)
write.csv(pred_rf_test, "./dta/pred_rf_test.csv", row.names = FALSE)


rf_mse <- table(pred_rf, as.matrix(test$label))
